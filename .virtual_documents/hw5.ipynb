# Initialize Otter
import otter
grader = otter.Notebook("hw5.ipynb")

















import pandas as pd

from sklearn.model_selection import train_test_split, cross_val_score, cross_validate
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier

from sklearn.feature_selection import RFECV
from sklearn.model_selection import cross_val_predict, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from scipy.stats import randint

















credit_card_df = pd.read_csv("./data/UCI_Credit_Card.csv", index_col=0)
credit_card_df.head()











train_df, test_df = train_test_split(credit_card_df, train_size=0.7, random_state=123)

X_train = credit_card_df.drop(columns="default.payment.next.month")
y_train = credit_card_df["default.payment.next.month"]

X_test = credit_card_df.drop(columns="default.payment.next.month")
y_test = credit_card_df["default.payment.next.month"]

display(train_df)
display(test_df)














train_df.info()





train_df.describe()





corr_map = train_df.corr()
plt.figure(figsize = (25, 25))
sns.heatmap(corr_map, annot = True, cmap = plt.cm.Blues)
plt.title('Correlation map of Features')
plt.show()





train_df['default.payment.next.month'].value_counts(normalize = True)


plt.hist(train_df['default.payment.next.month'], bins = 10)
plt.xlabel('default.payment.next.month')
plt.ylabel('Count')
plt.title('Target Class Distribution in Dataset')
plt.show()


























# Define column types
numeric_features = [
    'LIMIT_BAL', 'AGE',
    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'
]

categorical_features = ['SEX', 'MARRIAGE']

payment_status_features = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']

education_feature = ['EDUCATION']

# Create the column transformer
preprocessor = make_column_transformer(
    # Standard scaling for numeric features
    (StandardScaler(), numeric_features),

    # One-hot encoding for binary features
    (OneHotEncoder(drop="if_binary"), categorical_features),
    
    # Ordinal encoding for payment status
    (OrdinalEncoder(
        categories=[sorted(np.arange(-2, 9))] * len(payment_status_features),
        handle_unknown='use_encoded_value',
        unknown_value=-999
    ), payment_status_features),
    
    # Ordinal encoding for education - using -999 as unknown value
    (OrdinalEncoder(
        categories=[[0, 1, 2, 3, 4, 5, 6]],
        handle_unknown='use_encoded_value',
        unknown_value=-999
    ), education_feature)
)











results_dict = {}

dummy = DummyClassifier(strategy="most_frequent", random_state=42)
pipe_dc = make_pipeline(preprocessor, dummy)

cv_results = cross_validate(
    pipe_dc,
    X_train,
    y_train,
    cv=5,
    scoring=['accuracy', 'f1'],
    return_train_score=True
)

cv_results_df = pd.DataFrame(cv_results)
cv_results_df














C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]

results_dict = {}

for C in C_values:
    lr_pipe = make_pipeline(
        preprocessor,
        LogisticRegression(C=C, random_state=42, max_iter=2000)
    )
    
    cv_results = cross_validate(
        lr_pipe, 
        X_train,
        y_train,
        cv=5,
        scoring=['accuracy', 'f1'],
        return_train_score=True
    )
    
    results_dict[C] = {
        'train_accuracy': cv_results['train_accuracy'].mean(),
        'train_std': cv_results['train_accuracy'].std(),
        'cv_accuracy': cv_results['test_accuracy'].mean(),
        'cv_std': cv_results['test_accuracy'].std(),
        'fit_time': cv_results['fit_time'].mean(),
        'score_time': cv_results['score_time'].mean(),
        'train_f1': cv_results['train_f1'].mean(),
        'test_f1': cv_results['test_f1'].mean(),
    }
results_df = pd.DataFrame(results_dict).T
results_df





lr_pipe = make_pipeline(
        preprocessor,
        LogisticRegression(C=100, random_state=42, max_iter=2000)
    )

predictions = cross_val_predict(lr_pipe, X_train, y_train)
print(classification_report(y_train, predictions, target_names=["0", "1"], digits=4))














model_results = {}

# RandomForest
rf_pipe = make_pipeline(
    preprocessor,
    RandomForestClassifier(n_estimators=100, random_state=42)
)

# GradientBoost
gb_pipe = make_pipeline(
    preprocessor,
    GradientBoostingClassifier(n_estimators=100, random_state=42)
)

# SVM
svm_pipe = make_pipeline(
    preprocessor,
    SVC(random_state=42)
)

models = {
    'Random Forest': rf_pipe,
    'Gradient Boosting': gb_pipe,
    'SVM': svm_pipe
}

for name, model in models.items():
    cv_results = cross_validate(
        model,
        X_train,
        y_train,
        cv=5,
        scoring=['accuracy', 'f1'],
        return_train_score=True,
        n_jobs=-1
    )
    
    model_results[name] = {
        'train_accuracy': cv_results['train_accuracy'].mean(),
        'train_std': cv_results['train_accuracy'].std(),
        'cv_accuracy': cv_results['test_accuracy'].mean(),
        'cv_std': cv_results['test_accuracy'].std(),
        'fit_time': cv_results['fit_time'].mean(),
        'score_time': cv_results['score_time'].mean(),
        'train_f1': cv_results['train_f1'].mean(),
        'test_f1': cv_results['test_f1'].mean(),
    }

results_df = pd.DataFrame(model_results).T
results_df














X_train_scaled = preprocessor.fit_transform(X_train)

feature_names = (
    numeric_features +
    preprocessor.named_transformers_['onehotencoder'].get_feature_names_out(categorical_features).tolist() +
    payment_status_features +
    education_feature
)

rfe_cv = RFECV(
    LogisticRegression(max_iter=2000),
    scoring="f1",
    cv=10,
)

rfe_cv.fit(X_train_scaled, y_train)

selected_features = [name for name, selected in zip(feature_names, rfe_cv.support_) if selected]
selected_features


rfe_pipe = make_pipeline(
    StandardScaler(),
    rfe_cv,
)

pd.DataFrame(cross_validate(rfe_pipe, X_train, y_train, scoring=['accuracy', 'f1'], return_train_score=True)).mean()














lr_pipe = make_pipeline(
        preprocessor,
        LogisticRegression(max_iter=2000)
)

param_grid = {
    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
}

gs = GridSearchCV(
    lr_pipe, param_grid, n_jobs = -1, cv = 5, scoring=['accuracy', "f1"], refit='f1', return_train_score = True
)

gs.fit(X_train, y_train)

pd.DataFrame(gs.cv_results_).set_index('rank_test_f1').sort_index().T


display(pd.DataFrame(gs.cv_results_)[[
    'mean_test_f1',
    'param_logisticregression__C',
    'mean_fit_time',
    'rank_test_f1',
    'std_test_f1',
]].set_index('rank_test_f1').sort_index().T)

display(pd.DataFrame(gs.cv_results_)[[
    'mean_test_accuracy',
    'param_logisticregression__C',
    'mean_fit_time',
    'rank_test_accuracy',
    'std_test_accuracy',
]].set_index('rank_test_accuracy').sort_index().T)


best_params = gs.best_params_
best_params


best_lr_pipe = make_pipeline(preprocessor, LogisticRegression(C=100))
best_lr_pipe.fit(X_train, y_train)
best_lr_pipe.score(X_train, y_train)





rf_pipe = make_pipeline(
    preprocessor,
    RandomForestClassifier(random_state=42)
)

rf_param_dist = {
    'randomforestclassifier__n_estimators': randint(50, 200),
    'randomforestclassifier__max_depth': [5, 10, None],
}

rf_random = RandomizedSearchCV(
    rf_pipe,
    rf_param_dist,
    n_iter=10,
    cv=5,
    n_jobs=-1,
    scoring=["accuracy", "f1"],
    refit="f1",
    random_state=42,
    return_train_score=True
)

rf_random.fit(X_train, y_train)

pd.DataFrame(rf_random.cv_results_).set_index('rank_test_f1').sort_index().T


display(pd.DataFrame(rf_random.cv_results_)[[
    'mean_test_f1',
    'param_randomforestclassifier__max_depth',
    'param_randomforestclassifier__n_estimators',
    'mean_fit_time',
    'rank_test_f1',
    'std_test_f1',
]].set_index('rank_test_f1').sort_index().T)

display(pd.DataFrame(rf_random.cv_results_)[[
    'mean_test_accuracy',
    'param_randomforestclassifier__max_depth',
    'param_randomforestclassifier__n_estimators',
    'mean_fit_time',
    'rank_test_accuracy',
    'std_test_accuracy',
]].set_index('rank_test_accuracy').sort_index().T)


best_params = rf_random.best_params_
best_params


best_rf_pipe = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=171))
best_rf_pipe.fit(X_train, y_train)
best_rf_pipe.score(X_train, y_train)





gb_pipe = make_pipeline(
    preprocessor,
    GradientBoostingClassifier(random_state=42)
)

gb_param_dist = {
    'gradientboostingclassifier__n_estimators': randint(50, 200),
    'gradientboostingclassifier__max_depth': [3, 5, 7]
}

gb_random = RandomizedSearchCV(
    gb_pipe,
    gb_param_dist,
    n_iter=10,
    cv=5,
    n_jobs=-1,
    scoring=["accuracy", "f1"],
    refit="f1",
    random_state=42,
    return_train_score=True
)

gb_random.fit(X_train, y_train)

pd.DataFrame(gb_random.cv_results_).set_index('rank_test_f1').sort_index().T


display(pd.DataFrame(gb_random.cv_results_)[[
    'mean_test_f1',
    'param_gradientboostingclassifier__max_depth',
    'param_gradientboostingclassifier__n_estimators',
    'mean_fit_time',
    'rank_test_f1',
    'std_test_f1',
]].set_index('rank_test_f1').sort_index().T)

display(pd.DataFrame(gb_random.cv_results_)[[
    'mean_test_accuracy',
    'param_gradientboostingclassifier__max_depth',
    'param_gradientboostingclassifier__n_estimators',
    'mean_fit_time',
    'rank_test_accuracy',
    'std_test_accuracy',
]].set_index('rank_test_accuracy').sort_index().T)


best_params = gb_random.best_params_
best_params


best_gb_pipe = make_pipeline(preprocessor, GradientBoostingClassifier(max_depth=3, n_estimators=149))
best_gb_pipe.fit(X_train, y_train)
best_gb_pipe.score(X_train, y_train)














best_rf = make_pipeline(
    preprocessor,
    RandomForestClassifier(
        n_estimators=87,
        max_depth=10,
        random_state=42
    )
)

best_rf.fit(X_train, y_train)

preprocessed_features = (
    preprocessor.get_feature_names_out()
)

rf_importances = best_rf.named_steps['randomforestclassifier'].feature_importances_

importance_df = pd.DataFrame({
    'feature': preprocessed_features,
    'importance': rf_importances
})

importance_df = importance_df.sort_values('importance', ascending=False)

importance_df

















...


...


...


...


...


...


...


...


...


...


...














...


...
























